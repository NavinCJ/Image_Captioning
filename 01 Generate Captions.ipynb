{"cells":[{"cell_type":"code","source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"metadata":{"id":"I2bzMU9gp3AU"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7aG4mFIOlHq_"},"outputs":[],"source":["# Image captioning with visual attention\n","import tensorflow as tf\n","import collections\n","import random\n","import numpy as np\n","import pandas as pd\n","from google.colab import drive\n","import os\n","import time\n","import json\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yPW5c6xilwmE"},"outputs":[],"source":["from PIL import Image\n","## Download and prepare the MS-COCO dataset\n","# Download caption annotation files -- WE ARE DOWNLOADING ONLY CAPTIONS FROM THE JSON FILES\n","annotation_folder = '/annotations/'\n","if not os.path.exists(os.path.abspath('.') + annotation_folder):\n","\n","  #Download MS COCO dataset from the URL and save locally in 'captions.zip' and ALSO EXTRACT THE CONTENTS!  extract=True\n","  annotation_zip = tf.keras.utils.get_file('captions.zip', cache_subdir=os.path.abspath('.'),\n","                                           origin='http://images.cocodataset.org/annotations/annotations_trainval2014.zip', extract=True)\n","\n","  #Full path of the file \"captions_train2014.json\" from the extracted location\n","  annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n","\n","  #Delete the original zip file\n","  os.remove(annotation_zip)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bIPZ4CzEmZh7"},"outputs":[],"source":["print(os.path.abspath('.') + annotation_folder)\n","!pwd\n","print(annotation_zip)\n","!ls -al /content/annotations/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"btjX7aGTwyM1"},"outputs":[],"source":["# Download image files\n","image_folder = '/train2014/'\n","if not os.path.exists(os.path.abspath('.') + image_folder):\n","  image_zip = tf.keras.utils.get_file('train2014.zip', cache_subdir=os.path.abspath('.'), origin='http://images.cocodataset.org/zips/train2014.zip', extract=True)\n","  PATH = os.path.dirname(image_zip) + image_folder\n","  os.remove(image_zip)\n","else:\n","  PATH = os.path.abspath('.') + image_folder\n","\n","print(\"Location of the images:\" + PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ix46w1aR2Ifg"},"outputs":[],"source":["#All images are available in this path: /content/train2014/\n","#!ls -al $PATH\n","!ls -al /content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tf_m99gG4jHd"},"outputs":[],"source":["# CAPTIONS PREPROCESSING\n","with open(annotation_file, 'r') as f:\n","  annotations = json.load(f)\n","\n","# Group all captions together having the same image ID.  DEFAULT DICT CREATES KEY IF IT DOES NOT EXIST!\n","# So this will have IMAGE PATH as KEY and an ARRAY OF CAPTIONS.\n","image_path_to_caption = collections.defaultdict(list)\n","\n","for val in annotations['annotations']:\n","  caption = f\"<start> {val['caption']} <end>\"\n","  image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n","  image_path_to_caption[image_path].append(caption)\n","\n","#CONTAINS ONLY IMAGE PATHS\n","image_paths = list(image_path_to_caption.keys())\n","random.shuffle(image_paths)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mr2BmHmi44IS"},"outputs":[],"source":["print(caption)\n","print(image_path)\n","print(len(image_path_to_caption.keys()))\n","print(image_path_to_caption.get('/content/train2014/COCO_train2014_000000133071.jpg'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R7msrMwGAAWs"},"outputs":[],"source":["# Select the first 6000 image_paths from the shuffled set.??  SHOULD BE 10000 IMAGES\n","# Approximately each image id has 5 captions associated with it, so that will\n","# lead to 30,000 examples.\n","\n","train_image_paths = image_paths[:800]\n","print(\"Total Images:\" + str(len(image_paths)))\n","print(\"Selected Images:\" + str(len(train_image_paths)))\n","train_captions = []\n","img_name_vector = []\n","\n","for image_path in train_image_paths:\n","  caption_list = image_path_to_caption[image_path]\n","  train_captions.extend(caption_list)   #extend IS adding one list to another list.\n","  img_name_vector.extend([image_path] * len(caption_list))  #ADD IMAGE PATH THAT MANY TIMES DEPENDING UPON HOW MANY CAPTIONS WAS AVAILABLE FOR THAT IMAGE\n","\n","print(\"Training Captions SIZE:\" + str(len(train_captions)))\n","print(\"Training IMAGE SIZE:\" + str(len(img_name_vector)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7BPKriw7Dh61"},"outputs":[],"source":["## Preprocess the images using InceptionV3\n","def load_image(image_path):\n","  img = tf.io.read_file(image_path)\n","  img = tf.io.decode_jpeg(img, channels=3)\n","  img = tf.keras.layers.Resizing(299, 299)(img)\n","  img = tf.keras.applications.inception_v3.preprocess_input(img)\n","  return img, image_path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q7SreSKPDuw0"},"outputs":[],"source":["## Initialize InceptionV3 and load the pretrained Imagenet weights\n","## Inception v3[1][2] is a convolutional neural network for assisting in image analysis and object detection.\n","#Initialize InceptionV3 and load the pretrained Imagenet weights Now you'll create a tf.keras model\n","#where the output layer is the last convolutional layer in the InceptionV3 architecture. The shape of the output of this layer is ```8x8x2048```.\n","#Our goal is, use the pretrained model, and run our fresh images and save the generated output vector to disk.\n","#You use the last convolutional layer because you are using attention in this example. You don't perform this initialization during training because it could become a bottleneck.\n","#You forward each image through the network and store the resulting vector in a dictionary (image_name --> feature_vector).\n","#After all the images are passed through the network, you pickle the dictionary and save it to disk.\n","\n","image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')   #WE PULL THE EXISTING MODEL\n","new_input = image_model.input     #NEED TO FIND WHY WE DO THIS??\n","hidden_layer = image_model.layers[-1].output   ##NEED TO FIND WHY WE DO THIS??\n","image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xPZXmvs1Eyo6"},"outputs":[],"source":["## Caching the features extracted from InceptionV3\n","from tqdm import tqdm\n","\n","#REFRESH: img_name_vector - LIST OF ALL IMAGES; train_captions - LIST OF ALL CAPTIONS\n","# Get unique images\n","encode_train = sorted(set(img_name_vector))\n","\n","#Creates a Dataset whose elements are slices of the given tensors.\n","#The given tensors are sliced along their first dimension. This operation preserves the structure of the input tensors,\n","#removing the first dimension of each tensor and using it as the dataset dimension.\n","#All input tensors must have the same size in their first dimensions.\n","#https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices\n","image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n","image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n","\n","print(\"RANDOMLY PICKED 10K Image Dataset WHICH IS EXTENDED TO MATCH CAPTIONS COUNT:\" + str(len(img_name_vector)))\n","print(\"Image Dataset (UNIQUE IMAGES):\" + str(len(encode_train)))\n","print(\"Image Dataset SLICED (image_dataset):\" + str(len(image_dataset)))\n","\n","#SLICED DATASET WAS SHOWING 625.  THIS IS NOTHING BUG 10000 / 16 (BATCH SIZE) = 625."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KVAejhUlKRUp"},"outputs":[],"source":["import cv2\n","#cv2.IMREAD_COLOR : Loads a color image. Any transparency of image will be neglected. It is the default flag.\n","#cv2.IMREAD_GRAYSCALE : Loads image in grayscale mode.\n","#cv2.IMREAD_UNCHANGED : Loads image as such including alpha channel.\n","#These flags can be used directly instead of using the integers 1, 0 and -1 respectively.\n","img_color = cv2.imread(encode_train[0],1)\n","\n","from matplotlib import pyplot as plt\n","plt.imshow(img_color)\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hzKFyXC5HhZZ"},"outputs":[],"source":["#RECALL ABOVE THAT image_dataset size was 625.  So for each of the image, we are trying to extract the features\n","# and SAVING it back to the same folder /content/train2014/....*.npy extension\n","#. npy format is the standard binary file format in NumPy for persisting a single arbitrary NumPy array on disk.\n","\n","for img, path in tqdm(image_dataset):\n","  batch_features = image_features_extract_model(img)\n","  batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n","\n","  for bf, p in zip(batch_features, path):\n","    path_of_feature = p.numpy().decode(\"utf-8\")\n","    np.save(path_of_feature, bf.numpy())\n","\n","#WITH THIS, WE HAVE SUCCESSFULLY EXTRACTED FEATURE VECTORS OF THE 625 IMAGES! --- NO BUDDY, IT WAS 625 X 16 batches = 10000 images."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cOyWqx35Yl8t"},"outputs":[],"source":["#-rw-r--r-- 1 root root  183030 Jun 10 19:43 COCO_train2014_000000581582.jpg\n","#-rw-r--r-- 1 root root  524416 Jun 10 22:16 COCO_train2014_000000581582.jpg.npy\n","# For all TRAIN files that participated, we have another file with .npy created!\n","!ls -al /content/train2014"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9wguPLqVskTh"},"outputs":[],"source":["# Showing some binary content\n","!cat /content/train2014/COCO_train2014_000000581904.jpg.npy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KKh-rVcZYAez"},"outputs":[],"source":["## Preprocess and tokenize the captions\n","caption_dataset = tf.data.Dataset.from_tensor_slices(train_captions)\n","\n","def standardize(inputs):\n","  inputs = tf.strings.lower(inputs)\n","  return tf.strings.regex_replace(inputs, r\"!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~\", \"\")\n","\n","# Max word count for a caption.\n","max_length = 50\n","# Use the top 7000 words for a vocabulary.\n","vocabulary_size = 7000\n","\n","tokenizer = tf.keras.layers.TextVectorization(max_tokens=vocabulary_size, standardize=standardize, output_sequence_length=max_length)\n","\n","# Learn the vocabulary from the caption data.\n","tokenizer.adapt(caption_dataset)\n","# Create the tokenized vectors\n","cap_vector = caption_dataset.map(lambda x: tokenizer(x))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Pn_Eh0FagxK"},"outputs":[],"source":["# Create mappings for words to indices and indicies to words.\n","word_to_index = tf.keras.layers.StringLookup(mask_token=\"\", vocabulary=tokenizer.get_vocabulary())\n","index_to_word = tf.keras.layers.StringLookup(mask_token=\"\", vocabulary=tokenizer.get_vocabulary(), invert=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pWH4V3Je0ZuR"},"outputs":[],"source":["## Split the data into training and testing\n","img_to_cap_vector = collections.defaultdict(list)\n","\n","#img_name_vector == RANDOMLY PICKED 10K Image Dataset WHICH IS EXTENDED TO MATCH CAPTIONS COUNT:50023\n","#cap_vector == IS the captions VECTOR created from train_captions and its SIZE also will be 50023\n","for img, cap in zip(img_name_vector, cap_vector):\n","  img_to_cap_vector[img].append(cap)  #WOW...again we are building the same dictionary with KEY as IMAGE and value will be ARRAY of CAPTIONS\n","\n","# Create training and validation sets using an 80-20 split randomly.\n","img_keys = list(img_to_cap_vector.keys())\n","random.shuffle(img_keys)\n","slice_index = int(len(img_keys)*0.8)\n","img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n","img_name_train = []\n","cap_train = []\n","\n","#training set\n","for imgt in img_name_train_keys:\n","  capt_len = len(img_to_cap_vector[imgt])\n","  img_name_train.extend([imgt] * capt_len)\n","  cap_train.extend(img_to_cap_vector[imgt])\n","\n","#validation set\n","img_name_val = []\n","cap_val = []\n","\n","for imgv in img_name_val_keys:\n","  capv_len = len(img_to_cap_vector[imgv])\n","  img_name_val.extend([imgv] * capv_len)\n","  cap_val.extend(img_to_cap_vector[imgv])\n","\n","len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJhBcf2k4RtR"},"outputs":[],"source":["## STORY SO FAR!!!\n","## WE HAVE PICKED 10K RANDOM IMAGES AND ITS CAPTIONS, SO TOTAL IS 10K x 5 = 50K\n","## FOR THOSE 10K IMAGES, FEATURES VECTORS ARE EXTRACTED AND SAVED AS .NPY FILES\n","## FOR THOSE 50K CAPTIONS, TEXT VECTORS ARE GENERATED.\n","## NOW, WE ARE SPLITTING THE 50K INTO TRAINING AND VALIDATION SETS! TRAINING=40K; VALIDATION=10K\n","\n","## Create a tf.data dataset for training\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 1000\n","embedding_dim = 256\n","units = 512\n","num_steps = len(img_name_train) // BATCH_SIZE\n","# Shape of the vector extracted from InceptionV3 is (64, 2048)\n","# These two variables represent that vector shape\n","features_shape = 2048\n","attention_features_shape = 64\n","\n","# Load the numpy files\n","def map_func(img_name, cap):\n","  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n","  return img_tensor, cap\n","\n","dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n","\n","# Use map to load the numpy files in parallel\n","dataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int64]),num_parallel_calls=tf.data.AUTOTUNE)\n","\n","# Shuffle and batch\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fnxb62Xr85Qr"},"outputs":[],"source":["class BahdanauAttention(tf.keras.Model):\n","\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(units)\n","    self.W2 = tf.keras.layers.Dense(units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","  def call(self, features, hidden):\n","    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n","    # hidden shape == (batch_size, hidden_size)\n","    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n","    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n","    # attention_hidden_layer shape == (batch_size, 64, units)\n","    attention_hidden_layer = (tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)))\n","    # score shape == (batch_size, 64, 1)\n","    # This gives you an unnormalized score for each image feature.\n","    score = self.V(attention_hidden_layer)\n","    # attention_weights shape == (batch_size, 64, 1)\n","    attention_weights = tf.nn.softmax(score, axis=1)\n","    # context_vector shape after sum == (batch_size, hidden_size)\n","    context_vector = attention_weights * features\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","    return context_vector, attention_weights\n","\n","class CNN_Encoder(tf.keras.Model):\n","\n","  # Since you have already extracted the features and dumped it\n","  # This encoder passes those features through a Fully connected layer\n","  def __init__(self, embedding_dim):\n","    super(CNN_Encoder, self).__init__()\n","    # shape after fc == (batch_size, 64, embedding_dim)\n","    self.fc = tf.keras.layers.Dense(embedding_dim)\n","\n","  def call(self, x):\n","    x = self.fc(x)\n","    x = tf.nn.relu(x)\n","    return x\n","\n","class RNN_Decoder(tf.keras.Model):\n","\n","  def __init__(self, embedding_dim, units, vocab_size):\n","    super(RNN_Decoder, self).__init__()\n","    self.units = units\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(self.units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n","    self.fc1 = tf.keras.layers.Dense(self.units)\n","    self.fc2 = tf.keras.layers.Dense(vocab_size)\n","    self.attention = BahdanauAttention(self.units)\n","\n","  def call(self, x, features, hidden):\n","    # defining attention as a separate model\n","    context_vector, attention_weights = self.attention(features, hidden)\n","    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","    x = self.embedding(x)\n","    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","    # passing the concatenated vector to the GRU\n","    output, state = self.gru(x)\n","    # shape == (batch_size, max_length, hidden_size)\n","    x = self.fc1(output)\n","    # x shape == (batch_size * max_length, hidden_size)\n","    x = tf.reshape(x, (-1, x.shape[2]))\n","    # output shape == (batch_size * max_length, vocab)\n","    x = self.fc2(x)\n","    return x, state, attention_weights\n","\n","  def reset_state(self, batch_size):\n","    return tf.zeros((batch_size, self.units))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SJmlRSwF-oAi"},"outputs":[],"source":["encoder = CNN_Encoder(embedding_dim)\n","decoder = RNN_Decoder(embedding_dim, units, tokenizer.vocabulary_size())\n","#optimizer = tf.keras.optimizers() ---------------------->> WAS THROWING ERROR ---TypeError: 'module' object is not callable\n","optimizer = tf.keras.optimizers.Adam()\n","train_loss = tf.keras.losses.SparseCategoricalCrossentropy(name='train_loss', from_logits=True, reduction='none')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n","test_loss = tf.keras.losses.SparseCategoricalCrossentropy(name='test_loss', from_logits=True, reduction='none')\n","test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n","\n","def train_loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = train_loss(real, pred)\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","  return tf.reduce_mean(loss_)\n","\n","def train_accuracy_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  acc_ = train_accuracy(real, pred)\n","  mask = tf.cast(mask, dtype=acc_.dtype)\n","  acc_ *= mask\n","  return tf.reduce_mean(acc_)\n","\n","def test_loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = test_loss(real, pred)\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","  return tf.reduce_mean(loss_)\n","\n","def test_accuracy_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  acc_ = test_accuracy(real, pred)\n","  mask = tf.cast(mask, dtype=acc_.dtype)\n","  acc_ *= mask\n","  return tf.reduce_mean(acc_)\n","\n","## Checkpoint\n","checkpoint_path = \"./checkpoints/train\"\n","ckpt = tf.train.Checkpoint(encoder=encoder, decoder=decoder, optimizer=optimizer)\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","start_epoch = 0\n","\n","if ckpt_manager.latest_checkpoint:\n","  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n","  # restoring the latest checkpoint in checkpoint_path\n","  ckpt.restore(ckpt_manager.latest_checkpoint)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GRodH309_fZa"},"outputs":[],"source":["# adding this in a separate cell because if you run the training cell\n","# many times, the loss_plot array will be reset\n","loss_plot = []\n","accuracy_plot = []\n","\n","@tf.function\n","\n","def train_step(img_tensor, target):\n","  loss = 0\n","  accuracy = 0\n","  # initializing the hidden state for each batch\n","  # because the captions are not related from image to image\n","  hidden = decoder.reset_state(batch_size=target.shape[0])\n","  dec_input = tf.expand_dims([word_to_index('<start>')] * target.shape[0], 1)\n","\n","  with tf.GradientTape() as tape:\n","    features = encoder(img_tensor)\n","\n","    for i in range(1, target.shape[1]):\n","      # passing the features through the decoder\n","      predictions, hidden, _ = decoder(dec_input, features, hidden)\n","      loss += train_loss_function(target[:, i], predictions)\n","      accuracy += train_accuracy_function(target[:, i], predictions)\n","      # using teacher forcing\n","      dec_input = tf.expand_dims(target[:, i], 1)\n","\n","  total_loss = (loss / int(target.shape[1]))\n","  total_accuracy = (accuracy / int(target.shape[1]))\n","  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n","\n","  gradients = tape.gradient(loss, trainable_variables)\n","  optimizer.apply_gradients(zip(gradients, trainable_variables))\n","  return loss, total_loss, accuracy, total_accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nCQr3ONEMgiC"},"outputs":[],"source":["print(dataset)\n","#print(tf.shape(dataset))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P7pcqpuODvsn"},"outputs":[],"source":["EPOCHS = 100\n","for epoch in range(start_epoch, EPOCHS):\n","  start = time.time()\n","  total_loss = 0\n","  total_accuracy = 0\n","\n","  for (batch, (img_tensor, target)) in enumerate(dataset):\n","    batch_loss, t_loss, batch_accuracy, t_accuracy = train_step(img_tensor, target)\n","    total_loss += t_loss\n","    total_accuracy += t_accuracy\n","\n","    if batch % 100 == 0:\n","      average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n","      average_batch_accuracy = batch_accuracy.numpy()/int(target.shape[1])\n","      print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n","      print(f'Epoch {epoch+1} Batch {batch} Accuracy {average_batch_accuracy:.4f}')\n","\n","  # storing the epoch end loss value to plot later\n","  loss_plot.append(total_loss / num_steps)\n","  accuracy_plot.append(total_accuracy / num_steps)\n","\n","  if epoch % 5 == 0:\n","    ckpt_manager.save()\n","\n","  print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n","  print(f'Epoch {epoch+1} Accuracy {total_accuracy/num_steps:.6f}')\n","  print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n","\n","plt.plot(loss_plot)\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Loss Plot')\n","plt.show()\n","plt.plot(accuracy_plot)\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.title('Accuracy Plot')\n","plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EXilp4b6Fb4J"},"outputs":[],"source":["## Caption!\n","def evaluate(image):\n","\n","  attention_plot = np.zeros((max_length, attention_features_shape))\n","  hidden = decoder.reset_state(batch_size=1)\n","  temp_input = tf.expand_dims(load_image(image)[0], 0)\n","  img_tensor_val = image_features_extract_model(temp_input)\n","  img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n","  features = encoder(img_tensor_val)\n","  dec_input = tf.expand_dims([word_to_index('<start>')], 0)\n","  result = []\n","\n","  for i in range(max_length):\n","    predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n","    attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n","    predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n","    predicted_word = tf.compat.as_text(index_to_word(predicted_id).numpy())\n","    result.append(predicted_word)\n","\n","    if predicted_word == '<end>':\n","      return result, attention_plot\n","    dec_input = tf.expand_dims([predicted_id], 0)\n","\n","  attention_plot = attention_plot[:len(result), :]\n","  return result, attention_plot\n","\n","def plot_attention(image, result, attention_plot):\n","\n","  temp_image = np.array(Image.open(image))\n","  fig = plt.figure(figsize=(10, 10))\n","  len_result = len(result)\n","\n","  for i in range(len_result):\n","    temp_att = np.resize(attention_plot[i], (8, 8))\n","    grid_size = max(int(np.ceil(len_result/2)), 2)\n","    ax = fig.add_subplot(grid_size, grid_size, i+1)\n","    ax.set_title(result[i])\n","    img = ax.imshow(temp_image)\n","    ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n","\n","  plt.tight_layout()\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5rmqnEwJGhNS"},"outputs":[],"source":["# captions on the validation set\n","rid = np.random.randint(0, len(img_name_val))\n","image = img_name_val[rid]\n","real_caption = ' '.join([tf.compat.as_text(index_to_word(i).numpy()) for i in cap_val[rid] if i not in [0]])\n","result, attention_plot = evaluate(image)\n","print('Real Caption:', real_caption)\n","print('Prediction Caption:', ' '.join(result))\n","plot_attention(image, result, attention_plot)\n","\n","Image.open(image)\n"]},{"cell_type":"code","source":["image_url = 'https://tensorflow.org/images/surf.jpg'\n","image_extension = image_url[-4:]\n","image_path = tf.keras.utils.get_file('image'+image_extension, origin=image_url)\n","result, attention_plot = evaluate(image_path)\n","print('Prediction Caption:', ' '.join(result))\n","plot_attention(image_path, result, attention_plot)\n","\n","# opening the image\n","Image.open(image_path)"],"metadata":{"id":"fqf93xnn7WBI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZxeO1B3aHH0H"},"outputs":[],"source":["### GENERATING SYNTHETIC IMAGES USING TEXT\n","## train_image_paths - Contains path of 10K images\n","drive.mount('/content/drive', force_remount=True)\n","captions_list = []\n","for image_path in train_image_paths:\n","  caption = image_path_to_caption[image_path][0]\n","  captions_list.append(caption)\n","\n","Images_Captions_df = pd.DataFrame({'ORIGINAL IMAGES': train_image_paths, 'CAPTIONS': captions_list})\n","Images_Captions_df.to_csv('/content/drive/My Drive/Colab Notebooks/_IMAGE CAPTIONING/IMAGES_CAPTIONS_DATA.csv')\n","\n","\n","###USE 10 Generate Synthetic Images.ipynb NOTEBOOK TO GENERATE SYNTHETIC IMAGES!!!!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7-WD74EcHlCL"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oVqen19bJYHy"},"outputs":[],"source":["## Base Model\n","#@title Base Model\n","options = model_and_diffusion_defaults()\n","options['use_fp16'] = has_cuda\n","options['timestep_respacing'] = '100'\n","model,diffusion = create_model_and_diffusion(**options)\n","model.eval()\n","\n","if has_cuda:\n","  model.convert_to_fp16 ( )\n","\n","model.to(device)\n","model.load_state_dict(load_checkpoint('base',device))\n","print('total base parameters', sum(x.numel() for x in model.parameters()))\n","\n","## Upsampler Model\n","options_up = model_and_diffusion_defaults_upsampler()\n","options_up['use_fp16'] = has_cuda\n","options_up['timestep_respacing'] = 'fast27'\n","model_up,diffusion_up = create_model_and_diffusion(**options_up)\n","model_up.eval()\n","\n","if has_cuda :\n","  model_up.convert_to_fp16()\n","\n","model_up.to(device)\n","model_up.load_state_dict(load_checkpoint('upsample', device))\n","print('total upsampler parameters', sum(x.numel() for x in model_up.parameters()))\n","\n","## CLIP Model\n","clip_model = create_clip_model(device = device)\n","clip_model.image_encoder.load_state_dict(load_checkpoint('clip/image-enc', device))\n","clip_model.text_encoder.load_state_dict(load_checkpoint('clip/text-enc', device))\n","\n","def images(batch:th.Tensor):\n","  scaled = ((batch+1)*127.5).round().clamp(0,255).to(th.uint8).cpu()\n","  reshaped = scaled.permute(2,0,3,1).reshape([batch.shape[2],-1,3])\n","  return np.array(Image.fromarray(reshaped.numpy()))\n","\n","## Prompt\n","# Sampling parameters\n","batch_size = 1\n","guidance_scale = 3.0\n","# Tune this parameter to control the sharpness of 256x256 images .\n","# A value of 1.0 is sharper , but sometimes results in grainy artifacts .\n","upsample_temp = 0.997\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hh1NdK-TTxhF"},"outputs":[],"source":["## Base Model Sample\n","#Create the text tokens to feed to the model.\n","for i in range(0, 10000):\n","  prompt = str(Images_Captions_df['CAPTIONS'][i])\n","  tokens = model.tokenizer.encode(prompt)\n","  tokens,mask = model.tokenizer.padded_tokens_and_mask(tokens,options['text_ctx'])\n","\n","  # Pack the tokens together into model kwargs .\n","  model_kwargs = dict(tokens = th.tensor([tokens]*batch_size , device = device),\n","  mask = th.tensor([mask]*batch_size , dtype=th.bool , device = device),)\n","  # Setup guidance function for CLIP model .\n","  cond_fn = clip_model.cond_fn([prompt]*batch_size , guidance_scale)\n","  #Sample from the base model .\n","  model.del_cache()\n","\n","  samples = diffusion.p_sample_loop (model, (batch_size, 3, options[\"image_size\"] , options[\"image_size\"]),\n","                                     device = device, clip_denoised = True, progress = True, model_kwargs = model_kwargs, cond_fn = cond_fn,)\n","  model.del_cache()\n","  tokens = model_up.tokenizer.encode(prompt)\n","  tokens, mask = model_up.tokenizer.padded_tokens_and_mask (tokens , options_up['text_ctx'])\n","\n","  # Create the model conditioning dict .\n","  model_kwargs = dict(\n","      # Low - res image to upsample .\n","      low_res = ((samples+1)*127.5).round()/127.5 -1 ,\n","      # Text tokens\n","      tokens = th.tensor([tokens]*batch_size , device = device),\n","      mask = th.tensor([mask]*batch_size, dtype = th.bool, device = device))\n","\n","  #Sample from the base model .\n","  model_up.del_cache()\n","  up_shape = (batch_size, 3, options_up[\"image_size\"], options_up[\"image_size\"])\n","  up_samples = diffusion_up.ddim_sample_loop(model_up, up_shape,\n","  noise = th.randn(up_shape, device = device)*upsample_temp, device = device, clip_denoised = True, progress = True, model_kwargs = model_kwargs, cond_fn = None)[:batch_size]\n","  model_up.del_cache()\n","\n","  # Show the output\n","  image_array = images(up_samples)\n","  image = Image.fromarray(image_array)\n","  image.save('/content/drive/My Drive/Colab Notebooks/_IMAGE CAPTIONING/SYNTHETIC_IMAGES/'+str(i)+'.jpg')\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN2nDBNwLgbdE3m530GpQM4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}