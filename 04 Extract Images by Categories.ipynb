{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMxeO1EIPpbspTVdtj2I1dk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","#https://stackoverflow.com/questions/46910206/extracting-bounding-boxes-and-category-labels-in-ms-coco-dataset\n","#https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/coco.py\n","\n","from pycocotools.coco import COCO\n","from pycocotools.cocoeval import COCOeval\n","from pycocotools import mask as maskUtils\n","\n","import tensorflow as tf\n","import collections\n","import random\n","import re\n","import numpy as np\n","import os\n","import time\n","import json\n","from glob import glob\n","from PIL import Image\n","import pickle"],"metadata":{"id":"7CalBo0-Wgig"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#littleCo = COCO('/home/r.bohare/coco_data/annotations/instances_train2014.json')"],"metadata":{"id":"NJutuxRfWx3V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download caption annotation files\n","annotation_folder = '/annotations/'\n","if not os.path.exists(os.path.abspath('.') + annotation_folder):\n","  annotation_zip = tf.keras.utils.get_file('captions.zip',\n","                                          cache_subdir=os.path.abspath('.'),\n","                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n","                                          extract = True)\n","  annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n","  instances_file = os.path.dirname(annotation_zip)+'/annotations/instances_train2014.json'\n","  os.remove(annotation_zip)"],"metadata":{"id":"ecRQe5mnaI46"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#littleCo = COCO('/home/r.bohare/coco_data/annotations/instances_train2014.json')\n","littleCo = COCO(instances_file)"],"metadata":{"id":"_voA3TWaa8LM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["id_laptop = littleCo.getCatIds('laptop')\n","print(id_laptop) #73\n","\n","id_bag = littleCo.getCatIds('backpack')\n","print(id_bag) #27\n","\n","\"\"\"Extracting image ids corresponding to backpack and laptop images.\"\"\"\n","bag_img_ids = littleCo.getImgIds(catIds=[27])  #3924\n","laptop_img_ids = littleCo.getImgIds(catIds=[73])  #2475\n","print(\"IDs of bag images:\", len(bag_img_ids))\n","print(bag_img_ids)\n","print(\"IDs of laptop images:\", len(laptop_img_ids))\n","print(laptop_img_ids)"],"metadata":{"id":"ahpbRpWkbZj-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bag_laptop_img_ids = littleCo.getImgIds(catIds=[27,73])\n","print(\"IDs of bag and laptop images:\", len(bag_laptop_img_ids))\n","print(bag_laptop_img_ids)"],"metadata":{"id":"jNp9OKchdWlu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"Extracting image names corresponding to bag and laptop categories.\"\"\"\n","bag_imgs = littleCo.loadImgs(ids=bag_img_ids)\n","laptop_imgs = littleCo.loadImgs(ids=laptop_img_ids)\n","print(\"Bag images:\", bag_imgs)\n","print(\"Laptop images:\", laptop_imgs)"],"metadata":{"id":"0zSJXOhdeDW2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bag_img_names = [image['file_name'] for image in bag_imgs]\n","laptop_img_names = [image['file_name'] for image in laptop_imgs]\n","print(\"Bag Images:\", len(bag_img_names), bag_img_names[:5])\n","print(\"Laptop Images:\", len(laptop_img_names), laptop_img_names[:5])\n"],"metadata":{"id":"42DittXljHHk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# insert the list to the set\n","unique_images_set = set(bag_img_names)\n","print(\"Unique Bag Images count:\", len(unique_images_set))  #3924 so we are getting unique images"],"metadata":{"id":"GSkU-oL3cPBq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#EXPECTED: #/content/train2014/COCO_train2014_000000246608.jpg\n","prefix_str = '/content/train2014/'\n","\n","# Append suffix / prefix to strings in list\n","final_bag_img_names = [prefix_str + sub for sub in bag_img_names]\n","print(\"FINAL Bag Images:\", len(final_bag_img_names), final_bag_img_names[:5])"],"metadata":{"id":"nG-HRRNKeggu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from google.colab import drive\n","import numpy as np\n","# Mounting the drive\n","drive.mount('/content/drive')\n","\n","captions_list = []\n","for i_name in final_bag_img_names:\n","  captions_list.append(i_name)\n","\n","df = pd.DataFrame(captions_list, columns=[\"Captions\"])\n","df.to_csv('/content/drive/My Drive/Colab Notebooks/_IMAGE CAPTIONING/CAPTIONS_CSV/VAL_PREDICTED_CAPTIONS.csv', index=False)\n"],"metadata":{"id":"eW_j6co18Fw-"},"execution_count":null,"outputs":[]}]}