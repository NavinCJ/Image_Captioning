{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"B5M1bbrdYtXZ"},"outputs":[],"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\n","from PIL import Image\n","from IPython.display import display\n","import torch as th\n","import torch.nn as nn\n","import os\n","import pandas as pd\n","#from stackedGAN.model_creation import create_clip_model\n","#from stackedGAN.download import load_checkpoint\n","#from stackedGAN.model_creation import (create_model_and_diffusion, model_and_diffusion_defaults, model_and_diffusion_defaults_upsampler)\n","\n","#NAVIN: Below library never used, commenting!\n","#from glide_text2im.tokenizer.simple_tokenizer import SimpleTokenizer\n","\n","#https://github.com/openai/glide-text2im/blob/main/notebooks/clip_guided.ipynb\n","#https://github.com/openai/glide-text2im/blob/main/notebooks/text2im.ipynb\n","!pip install git+https://github.com/openai/glide-text2im\n","\n","#NAVIN-CORRECTED-VERSION\n","from glide_text2im.clip.model_creation import create_clip_model\n","from glide_text2im.download import load_checkpoint\n","from glide_text2im.model_creation import (create_model_and_diffusion, model_and_diffusion_defaults, model_and_diffusion_defaults_upsampler)\n","\n","\n","has_cuda = th.cuda.is_available()\n","device = th.device('cpu' if not has_cuda else 'cuda')\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FqtQ5YBCeIZn"},"outputs":[],"source":["## Base Model\n","#@title Base Model\n","options = model_and_diffusion_defaults()\n","options['use_fp16'] = has_cuda\n","options['timestep_respacing'] = '100'\n","model,diffusion = create_model_and_diffusion(**options)\n","model.eval()\n","\n","if has_cuda:\n","  model.convert_to_fp16 ( )\n","\n","model.to(device)\n","model.load_state_dict(load_checkpoint('base',device))\n","print('total base parameters', sum(x.numel() for x in model.parameters()))\n","\n","## Upsampler Model\n","options_up = model_and_diffusion_defaults_upsampler()\n","options_up['use_fp16'] = has_cuda\n","options_up['timestep_respacing'] = 'fast27'\n","model_up,diffusion_up = create_model_and_diffusion(**options_up)\n","model_up.eval()\n","\n","if has_cuda :\n","  model_up.convert_to_fp16()\n","\n","model_up.to(device)\n","model_up.load_state_dict(load_checkpoint('upsample', device))\n","print('total upsampler parameters', sum(x.numel() for x in model_up.parameters()))\n","\n","## CLIP Model\n","clip_model = create_clip_model(device = device)\n","clip_model.image_encoder.load_state_dict(load_checkpoint('clip/image-enc', device))\n","clip_model.text_encoder.load_state_dict(load_checkpoint('clip/text-enc', device))\n","\n","def images(batch:th.Tensor):\n","  scaled = ((batch+1)*127.5).round().clamp(0,255).to(th.uint8).cpu()\n","  reshaped = scaled.permute(2,0,3,1).reshape([batch.shape[2],-1,3])\n","  return np.array(Image.fromarray(reshaped.numpy()))\n","\n","## Prompt\n","# Sampling parameters\n","batch_size = 1\n","guidance_scale = 3.0\n","# Tune this parameter to control the sharpness of 256x256 images .\n","# A value of 1.0 is sharper , but sometimes results in grainy artifacts .\n","upsample_temp = 0.997\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xu57WpC1e3Ee"},"outputs":[],"source":["import os\n","#os.path.exists('/content/drive/My Drive/Colab Notebooks/_IMAGE CAPTIONING/SYNTHETIC_IMAGES/'+str(i)+'.jpg'):\n","os.path.exists('/content/drive/My Drive/Colab Notebooks/_IMAGE CAPTIONING/SYNTHETIC_IMAGES/'+'0.jpg')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wVBnVLlWesbZ"},"outputs":[],"source":["import pandas as pd\n","from google.colab import drive\n","import numpy as np\n","# Mounting the drive\n","drive.mount('/content/drive')\n","\n","Images_Captions_df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/_IMAGE CAPTIONING/IMAGES_CAPTIONS_DATA.csv')\n","\n","## Base Model Sample\n","#Create the text tokens to feed to the model.\n","for i in range(0, 10000):\n","  if (os.path.exists('/content/drive/My Drive/Colab Notebooks/_IMAGE CAPTIONING/SYNTHETIC_IMAGES/'+str(i)+'.jpg')) :\n","    continue\n","  prompt = str(Images_Captions_df['CAPTIONS'][i])\n","  tokens = model.tokenizer.encode(prompt)\n","  tokens,mask = model.tokenizer.padded_tokens_and_mask(tokens,options['text_ctx'])\n","\n","  # Pack the tokens together into model kwargs .\n","  model_kwargs = dict(tokens = th.tensor([tokens]*batch_size , device = device),\n","  mask = th.tensor([mask]*batch_size , dtype=th.bool , device = device),)\n","  # Setup guidance function for CLIP model .\n","  cond_fn = clip_model.cond_fn([prompt]*batch_size , guidance_scale)\n","  #Sample from the base model .\n","  model.del_cache()\n","\n","  samples = diffusion.p_sample_loop (model, (batch_size, 3, options[\"image_size\"] , options[\"image_size\"]),\n","                                     device = device, clip_denoised = True, progress = True, model_kwargs = model_kwargs, cond_fn = cond_fn,)\n","  model.del_cache()\n","  tokens = model_up.tokenizer.encode(prompt)\n","  tokens, mask = model_up.tokenizer.padded_tokens_and_mask (tokens , options_up['text_ctx'])\n","\n","  # Create the model conditioning dict .\n","  model_kwargs = dict(\n","      # Low - res image to upsample .\n","      low_res = ((samples+1)*127.5).round()/127.5 -1 ,\n","      # Text tokens\n","      tokens = th.tensor([tokens]*batch_size , device = device),\n","      mask = th.tensor([mask]*batch_size, dtype = th.bool, device = device))\n","\n","  #Sample from the base model .\n","  model_up.del_cache()\n","  up_shape = (batch_size, 3, options_up[\"image_size\"], options_up[\"image_size\"])\n","  up_samples = diffusion_up.ddim_sample_loop(model_up, up_shape,\n","  noise = th.randn(up_shape, device = device)*upsample_temp, device = device, clip_denoised = True, progress = True, model_kwargs = model_kwargs, cond_fn = None)[:batch_size]\n","  model_up.del_cache()\n","\n","  # Show the output\n","  image_array = images(up_samples)\n","  image = Image.fromarray(image_array)\n","  image.save('/content/drive/My Drive/Colab Notebooks/_IMAGE CAPTIONING/SYNTHETIC_IMAGES/'+str(i)+'.jpg')\n","\n"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNv5FLu/mAy6muGiTago0I6"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}