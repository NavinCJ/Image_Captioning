{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN+yw8ubrkGod8MaCOJ+uIr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\n","import tensorflow as tf\n","import collections\n","import random\n","import numpy as np\n","import pandas as pd\n","from google.colab import drive\n","import os\n","import time\n","import json\n","from matplotlib import pyplot as plt\n","from PIL import Image\n","\n","#Set the value here\n","TOTAL_SYNTHETIC_IMAGES = 842"],"metadata":{"id":"GziT_TTvPo_W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### LOADING THE SYNTHETIC IMAGES AND THEIR CAPTIONS FOR BUILDING THE IMAGE CAPTION GENERATOR\n","# Mounting the drive\n","drive.mount('/content/drive')\n","Synthetic_Images_Captions_df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/_IMAGE CAPTIONING/IMAGES_CAPTIONS_DATA.csv')\n","Synthetic_Images_Captions_df\n","train_captions = list(Synthetic_Images_Captions_df['CAPTIONS'])\n","img_name_vector = []\n","#for i in range(len(train_captions)):\n","for i in range(TOTAL_SYNTHETIC_IMAGES+1):\n","  syn_img_path = '/content/drive/My Drive/Colab Notebooks/_IMAGE CAPTIONING/SYNTHETIC_IMAGES/'+str(i)+'.jpg'\n","  img_name_vector.append(syn_img_path)\n","\n","print(\"SYNTHETIC Images loaded:\" + str(len(img_name_vector)))"],"metadata":{"id":"bMtHFPc-YszC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zYHRElglOiqp"},"outputs":[],"source":["## Preprocess the images using InceptionV3\n","def load_image(image_path):\n","  img = tf.io.read_file(image_path)\n","  img = tf.io.decode_jpeg(img, channels=3)\n","  img = tf.keras.layers.Resizing(299, 299)(img)\n","  img = tf.keras.applications.inception_v3.preprocess_input(img)\n","  return img, image_path\n","\n","## Initialize InceptionV3 and load the pretrained Imagenet weights\n","image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n","new_input = image_model.input\n","hidden_layer = image_model.layers[-1].output\n","image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n","\n","## Caching the features extracted from InceptionV3\n","from tqdm import tqdm   #tqdm/tqdm: A Fast, Extensible Progress Bar for Python\n","\n","# Get unique images\n","image_dataset = tf.data.Dataset.from_tensor_slices(img_name_vector)\n","image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n","\n","for img, path in tqdm(image_dataset):\n","  batch_features = image_features_extract_model(img)\n","  batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n","  for bf, p in zip(batch_features, path):\n","    path_of_feature = p.numpy().decode(\"utf-8\")\n","    np.save(path_of_feature, bf.numpy())\n","\n","## Preprocess and tokenize the captions\n","caption_dataset = tf.data.Dataset.from_tensor_slices(train_captions)\n","\n","def standardize(inputs):\n","  inputs = tf.strings.lower(inputs)\n","  return tf.strings.regex_replace(inputs, r\"!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~\", \"\")\n","\n","# Max word count for a caption.\n","max_length = 50\n","# Use the top 7000 words for a vocabulary.\n","vocabulary_size = 7000\n","\n","tokenizer = tf.keras.layers.TextVectorization(max_tokens=vocabulary_size, standardize=standardize, output_sequence_length=max_length)\n","\n","# Learn the vocabulary from the caption data.\n","tokenizer.adapt(caption_dataset)\n","# Create the tokenized vectors\n","cap_vector = caption_dataset.map(lambda x: tokenizer(x))\n","# Create mappings for words to indices and indicies to words.\n","word_to_index = tf.keras.layers.StringLookup(mask_token=\"\", vocabulary=tokenizer.get_vocabulary())\n","index_to_word = tf.keras.layers.StringLookup(mask_token=\"\", vocabulary=tokenizer.get_vocabulary(), invert=True)\n","\n","## Split the data into training and testing\n","img_to_cap_vector = collections.defaultdict(list)\n","for img, cap in zip(img_name_vector, cap_vector):\n","  img_to_cap_vector[img].append(cap)\n","\n","# Create training and validation sets using an 80-20 split randomly.\n","img_keys = list(img_to_cap_vector.keys())\n","random.shuffle(img_keys)\n","slice_index = int(len(img_keys)*0.8)\n","\n","img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n","img_name_train = []\n","cap_train = []\n","\n","for imgt in img_name_train_keys:\n","  capt_len = len(img_to_cap_vector[imgt])\n","  img_name_train.extend([imgt] * capt_len)\n","  cap_train.extend(img_to_cap_vector[imgt])\n","\n","img_name_val = []\n","cap_val = []\n","\n","for imgv in img_name_val_keys:\n","  capv_len = len(img_to_cap_vector[imgv])\n","  img_name_val.extend([imgv] * capv_len)\n","  cap_val.extend(img_to_cap_vector[imgv])\n","\n","len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)\n","\n","## Create a tf.data dataset for training\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 1000\n","embedding_dim = 256\n","units = 512\n","num_steps = len(img_name_train) // BATCH_SIZE\n","# Shape of the vector extracted from InceptionV3 is (64, 2048)\n","# These two variables represent that vector shape\n","features_shape = 2048\n","attention_features_shape = 64\n","\n","# Load the numpy files\n","def map_func(img_name, cap):\n","  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n","  return img_tensor, cap\n","\n","dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n","# Use map to load the numpy files in parallel\n","dataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int64]), num_parallel_calls=tf.data.AUTOTUNE)\n","\n","# Shuffle and batch\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n"]},{"cell_type":"code","source":["class BahdanauAttention(tf.keras.Model):\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(units)\n","    self.W2 = tf.keras.layers.Dense(units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","  def call(self, features, hidden):\n","    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n","    # hidden shape == (batch_size, hidden_size)\n","    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n","    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n","    # attention_hidden_layer shape == (batch_size, 64, units)\n","    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n","    self.W2(hidden_with_time_axis)))\n","    # score shape == (batch_size, 64, 1)\n","    # This gives you an unnormalized score for each image feature.\n","    score = self.V(attention_hidden_layer)\n","    # attention_weights shape == (batch_size, 64, 1)\n","    attention_weights = tf.nn.softmax(score, axis=1)\n","    # context_vector shape after sum == (batch_size, hidden_size)\n","    context_vector = attention_weights * features\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","    return context_vector, attention_weights\n","\n","class CNN_Encoder(tf.keras.Model):\n","  # Since you have already extracted the features and dumped it\n","  # This encoder passes those features through a Fully connected layer\n","  def __init__(self, embedding_dim):\n","    super(CNN_Encoder, self).__init__()\n","    # shape after fc == (batch_size, 64, embedding_dim)\n","    self.fc = tf.keras.layers.Dense(embedding_dim)\n","\n","  def call(self, x):\n","    x = self.fc(x)\n","    x = tf.nn.relu(x)\n","    return x\n","\n","class RNN_Decoder(tf.keras.Model):\n","  def __init__(self, embedding_dim, units, vocab_size):\n","    super(RNN_Decoder, self).__init__()\n","    self.units = units\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(self.units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n","    self.fc1 = tf.keras.layers.Dense(self.units)\n","    self.fc2 = tf.keras.layers.Dense(vocab_size)\n","    self.attention = BahdanauAttention(self.units)\n","\n","  def call(self, x, features, hidden):\n","    # defining attention as a separate model\n","    context_vector, attention_weights = self.attention(features, hidden)\n","    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","    x = self.embedding(x)\n","    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","    # passing the concatenated vector to the GRU\n","    output, state = self.gru(x)\n","    # shape == (batch_size, max_length, hidden_size)\n","    x = self.fc1(output)\n","    # x shape == (batch_size * max_length, hidden_size)\n","    x = tf.reshape(x, (-1, x.shape[2]))\n","    # output shape == (batch_size * max_length, vocab)\n","    x = self.fc2(x)\n","    return x, state, attention_weights\n","\n","  def reset_state(self, batch_size):\n","    return tf.zeros((batch_size, self.units))\n","\n"],"metadata":{"id":"CY8ziWXcPxwB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder = CNN_Encoder(embedding_dim)\n","decoder = RNN_Decoder(embedding_dim, units, tokenizer.vocabulary_size())\n","optimizer = tf.keras.optimizers.Adam()\n","train_loss = tf.keras.losses.SparseCategoricalCrossentropy(name='train_loss', from_logits=True, reduction='none')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n","test_loss = tf.keras.losses.SparseCategoricalCrossentropy(name='test_loss', from_logits=True, reduction='none')\n","test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n","\n","def train_loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = train_loss(real, pred)\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","  return tf.reduce_mean(loss_)\n","\n","def train_accuracy_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  acc_ = train_accuracy(real, pred)\n","  mask = tf.cast(mask, dtype=acc_.dtype)\n","  acc_ *= mask\n","  return tf.reduce_mean(acc_)\n","\n","def test_loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = test_loss(real, pred)\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","  return tf.reduce_mean(loss_)\n","\n","def test_accuracy_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  acc_ = test_accuracy(real, pred)\n","  mask = tf.cast(mask, dtype=acc_.dtype)\n","  acc_ *= mask\n","  return tf.reduce_mean(acc_)\n","\n","## Checkpoint\n","checkpoint_path = \"./checkpoints/train\"\n","ckpt = tf.train.Checkpoint(encoder=encoder, decoder=decoder, optimizer=optimizer)\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","start_epoch = 0\n","\n","if ckpt_manager.latest_checkpoint:\n","  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n","  # restoring the latest checkpoint in checkpoint_path\n","  ckpt.restore(ckpt_manager.latest_checkpoint)\n","\n","# adding this in a separate cell because if you run the training cell\n","# many times, the loss_plot array will be reset\n","\n","loss_plot = []\n","accuracy_plot = []\n","@tf.function\n","\n","def train_step(img_tensor, target):\n","  loss = 0\n","  accuracy = 0\n","  # initializing the hidden state for each batch\n","  # because the captions are not related from image to image\n","  hidden = decoder.reset_state(batch_size=target.shape[0])\n","  dec_input = tf.expand_dims([word_to_index('<start>')] * target.shape[0], 1)\n","\n","  with tf.GradientTape() as tape:\n","    features = encoder(img_tensor)\n","    for i in range(1, target.shape[1]):\n","      # passing the features through the decoder\n","      predictions, hidden, _ = decoder(dec_input, features, hidden)\n","      loss += train_loss_function(target[:, i], predictions)\n","      accuracy += train_accuracy_function(target[:, i], predictions)\n","      # using teacher forcing\n","      dec_input = tf.expand_dims(target[:, i], 1)\n","\n","  total_loss = (loss / int(target.shape[1]))\n","  total_accuracy = (accuracy / int(target.shape[1]))\n","  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n","  gradients = tape.gradient(loss, trainable_variables)\n","  optimizer.apply_gradients(zip(gradients, trainable_variables))\n","  return loss, total_loss, accuracy, total_accuracy"],"metadata":{"id":"2HNQio91Qj1k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 100\n","\n","for epoch in range(start_epoch, EPOCHS):\n","  start = time.time()\n","  total_loss = 0\n","  total_accuracy = 0\n","  for (batch, (img_tensor, target)) in enumerate(dataset):\n","    batch_loss, t_loss, batch_accuracy, t_accuracy = train_step(img_tensor, target)\n","    total_loss += t_loss\n","    total_accuracy += t_accuracy\n","    if batch % 100 == 0:\n","      average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n","      average_batch_accuracy = batch_accuracy.numpy()/int(target.shape[1])\n","      print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n","      print(f'Epoch {epoch+1} Batch {batch} Accuracy {average_batch_accuracy:.4f}')\n","\n","  # storing the epoch end loss value to plot later\n","  loss_plot.append(total_loss / num_steps)\n","  accuracy_plot.append(total_accuracy / num_steps)\n","\n","  if epoch % 5 == 0:\n","    ckpt_manager.save()\n","\n","  print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n","  print(f'Epoch {epoch+1} Accuracy {total_accuracy/num_steps:.6f}')\n","  print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n","\n","plt.plot(loss_plot)\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Loss Plot')\n","plt.show()\n","plt.plot(accuracy_plot)\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.title('Accuracy Plot')\n","plt.show()\n","\n","\n","def evaluate(image):\n","  attention_plot = np.zeros((max_length, attention_features_shape))\n","  hidden = decoder.reset_state(batch_size=1)\n","  temp_input = tf.expand_dims(load_image(image)[0], 0)\n","  img_tensor_val = image_features_extract_model(temp_input)\n","  img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n","  features = encoder(img_tensor_val)\n","  dec_input = tf.expand_dims([word_to_index('<start>')], 0)\n","  result = []\n","\n","  for i in range(max_length):\n","    predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n","    attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n","    predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n","    predicted_word = tf.compat.as_text(index_to_word(predicted_id).numpy())\n","    result.append(predicted_word)\n","\n","    if predicted_word == '<end>':\n","      return result, attention_plot\n","\n","    dec_input = tf.expand_dims([predicted_id], 0)\n","\n","  attention_plot = attention_plot[:len(result), :]\n","  return result, attention_plot\n","\n","def plot_attention(image, result, attention_plot):\n","  temp_image = np.array(Image.open(image))\n","  fig = plt.figure(figsize=(10, 10))\n","  len_result = len(result)\n","  for i in range(len_result):\n","    temp_att = np.resize(attention_plot[i], (8, 8))\n","    grid_size = max(int(np.ceil(len_result/2)), 2)\n","    ax = fig.add_subplot(grid_size, grid_size, i+1)\n","    ax.set_title(result[i])\n","    img = ax.imshow(temp_image)\n","    ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n","\n","  plt.tight_layout()\n","  plt.show()\n","\n","\n"],"metadata":{"id":"jQmIhQqXRrZb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# captions on the validation set\n","rid = np.random.randint(0, len(img_name_val))\n","print(\"Random Image ID:\" + str(rid))\n","image = img_name_val[rid]\n","real_caption = ' '.join([tf.compat.as_text(index_to_word(i).numpy()) for i in cap_val[rid] if i not in [0]])\n","result, attention_plot = evaluate(image)\n","print('Real Caption:', real_caption)\n","print('Prediction Caption:', ' '.join(result))\n","plot_attention(image, result, attention_plot)\n","\n","Image.open(image)\n","image_url = 'https://tensorflow.org/images/surf.jpg'\n","image_extension = image_url[-4:]\n","image_path = tf.keras.utils.get_file('image'+image_extension, origin=image_url)\n","result, attention_plot = evaluate(image_path)\n","print('Prediction Caption:', ' '.join(result))\n","plot_attention(image_path, result, attention_plot)\n","# opening the image\n","Image.open(image_path)\n"],"metadata":{"id":"ByY115B0eVXH"},"execution_count":null,"outputs":[]}]}