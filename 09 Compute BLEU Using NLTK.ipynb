{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/gcunhase/NLPMetrics/blob/master/notebooks/bleu.ipynb","timestamp":1686861204809}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"metadata":{"id":"vBM2xKonzgIR"},"execution_count":null,"outputs":[]},{"metadata":{"id":"cEvHsd4OnnO2"},"cell_type":"markdown","source":["## BLEU: BiLingual Evaluation Understudy\n","\n","*NLP evaluation metric used in Machine Translation tasks*\n","\n","*Suitable for measuring corpus level similarity*\n","\n","*$n$-gram comparison between words in candidate sentence and reference sentences*\n","\n","*Range: 0 (no match) to 1 (exact match)*"]},{"metadata":{"id":"_arqa6LRnzCL"},"cell_type":"markdown","source":["### 1. Libraries\n","*Install and import necessary libraries*\n"]},{"metadata":{"id":"xFOnk5JdnuYQ"},"cell_type":"code","source":["import nltk\n","import nltk.translate.bleu_score as bleu\n","\n","import math\n","import numpy\n","import os\n","\n","try:\n","  nltk.data.find('tokenizers/punkt')\n","except LookupError:\n","  nltk.download('punkt')"],"execution_count":null,"outputs":[]},{"metadata":{"id":"SVkfsYSZq_zn"},"cell_type":"markdown","source":["### 2. Dataset\n","*Array of words: candidate and reference sentences split into words*"]},{"metadata":{"id":"Dr9v92X0r9VM"},"cell_type":"code","source":["hyp = str('she read the book because she was interested in world history').split()\n","ref_a = str('she read the book because she was interested in world history').split()\n","ref_b = str('she was interested in world history because she read the book').split()"],"execution_count":null,"outputs":[]},{"metadata":{"id":"PQYjMHOgsyfT"},"cell_type":"markdown","source":["### 3. *Sentence* score calculation\n","*Compares 1 hypothesis (candidate or source sentence) with 1+ reference sentences, returning the highest score when compared to multiple reference sentences.*"]},{"metadata":{"id":"jXGCD-pi-jt5"},"cell_type":"code","source":["score_ref_a = bleu.sentence_bleu([ref_a], hyp)\n","print(\"Hyp and ref_a are the same: {}\".format(score_ref_a))\n","score_ref_b = bleu.sentence_bleu([ref_b], hyp)\n","print(\"Hyp and ref_b are different: {}\".format(score_ref_b))\n","score_ref_ab = bleu.sentence_bleu([ref_a, ref_b], hyp)\n","print(\"Hyp vs multiple refs: {}\".format(score_ref_ab))"],"execution_count":null,"outputs":[]},{"metadata":{"id":"NW9ZXSsSs6bE"},"cell_type":"markdown","source":["### 4. *Corpus* score calculation\n","*Compares 1 candidate document with multiple sentence and 1+ reference documents also with multiple sentences.*\n","\n","* Different than averaging BLEU scores of each sentence, it calculates the score by *\"summing the numerators and denominators for each hypothesis-reference(s) pairs before the division\"*"]},{"metadata":{"id":"XATgeqKPP02p"},"cell_type":"code","source":["score_ref_a = bleu.corpus_bleu([[ref_a]], [hyp])\n","print(\"1 document with 1 reference sentence: {}\".format(score_ref_a))\n","score_ref_a = bleu.corpus_bleu([[ref_a, ref_b]], [hyp])\n","print(\"1 document with 2 reference sentences: {}\".format(score_ref_a))\n","score_ref_a = bleu.corpus_bleu([[ref_a], [ref_b]], [hyp, hyp])\n","print(\"2 documents with 1 reference sentence each: {}\".format(score_ref_a))"],"execution_count":null,"outputs":[]},{"metadata":{"id":"hxgDToMctnTM"},"cell_type":"markdown","source":["### 5. BLEU-$n$\n","*In BLEU-$n$, $n$-gram scores can be obtained in both **sentence** and **corpus** calculations and they're indicated by the **weights** parameter.*\n","\n","* *weights*: length 4, where each index contains a weight corresponding to its respective $n$-gram.\n","* $n$-gram with $n \\in \\{1, 2, 3, 4\\}$\n","* $\\textit{weights}=(W_{N=1}, W_{N=2}, W_{N=3}, W_{N=4})$\n","\n"]},{"metadata":{"id":"0J2_E8zQP6K9"},"cell_type":"code","source":["score_1gram = bleu.sentence_bleu([ref_b], hyp, weights=(1,0,0,0))\n","score_2gram = bleu.sentence_bleu([ref_b], hyp, weights=(0,1,0,0))\n","score_3gram = bleu.sentence_bleu([ref_b], hyp, weights=(0,0,1,0))\n","score_4gram = bleu.sentence_bleu([ref_b], hyp, weights=(0,0,0,1))\n","print(\"N-grams: 1-{}, 2-{}, 3-{}, 4-{}\".format(score_1gram, score_2gram, score_3gram, score_4gram))"],"execution_count":null,"outputs":[]},{"metadata":{"id":"El1PaLtIDQyH"},"cell_type":"markdown","source":["* Cumulative N-grams: *by default, the score is calculatedby considering all $N$-grams equally in a geometric mean*"]},{"metadata":{"id":"ntJ1UkEaP-90"},"cell_type":"code","source":["score_ngram1 = bleu.sentence_bleu([ref_b], hyp)\n","score_ngram = bleu.sentence_bleu([ref_b], hyp, weights=(0.25,0.25,0.25,0.25))\n","score_ngram_geo = (11/11*9/10*6/9*4/8)**0.25\n","print(\"N-grams: {} = {} = \".format(score_ngram1, score_ngram, score_ngram_geo))"],"execution_count":null,"outputs":[]},{"metadata":{"id":"oq3WktXzV2I9"},"cell_type":"markdown","source":["### Further testing"]},{"metadata":{"id":"rdVRiZQc-ebC"},"cell_type":"code","source":["hyp = str('she read the book because she was interested in world history').split()\n","ref_a = str('she was interested in world history because she read the book').split()\n","hyp_b = str('the book she read was about modern civilizations.').split()\n","ref_b = str('the book she read was about modern civilizations.').split()\n","\n","score_a = bleu.sentence_bleu([ref_a], hyp)\n","score_b = bleu.sentence_bleu([ref_b], hyp_b)\n","score_ab = bleu.sentence_bleu([ref_a], hyp_b)\n","score_ba = bleu.sentence_bleu([ref_b], hyp)\n","score_ref_a = bleu.corpus_bleu([[ref_a], [ref_b]], [hyp, hyp_b])\n","average = (score_a+score_b)/2\n","corpus = math.pow((11+8)/19 * (9+7)/(17) * (6+6)/(9+6) * (4+5)/(8+5), 1/4)\n","print(\"Sent: {}, {}, {}, {} - Corpus {}, {}, {}\".format(score_a, score_b, score_ab, score_ba, score_ref_a, average, corpus))"],"execution_count":null,"outputs":[]}]}